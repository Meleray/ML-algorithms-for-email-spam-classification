{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coursework_graph.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mc0kAZgQrG6E",
        "oWvlKPVLmVwm"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8bpAMQNMREW",
        "outputId": "da90612a-a7d5-4780-a4a0-66a3f38b758e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNeAAveoMYpF",
        "outputId": "7a33bd67-0222-4975-941b-3986f9eb6b5a"
      },
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install tensor2tensor\n",
        "!pip install six\n",
        "import numpy as np\n",
        "import os\n",
        "import tarfile\n",
        "import zipfile\n",
        "import email\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import copy\n",
        "import functools\n",
        "from six.moves import range\n",
        "from tensor2tensor.layers import common_attention\n",
        "from tensor2tensor.layers import common_layers\n",
        "from tensor2tensor.models import transformer\n",
        "from tensor2tensor.utils import registry\n",
        "from tensor2tensor.utils import expert_utils\n",
        "import networkx as nx\n",
        "import io"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.12.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu111)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu111)\n",
            "Requirement already satisfied: tokenizers>=0.10.3 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.1)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: tensor2tensor in /usr/local/lib/python3.7/dist-packages (1.15.7)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (0.14.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (0.16.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (4.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (4.62.3)\n",
            "Requirement already satisfied: mesh-tensorflow in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (0.1.19)\n",
            "Requirement already satisfied: gevent in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (21.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (1.7.1)\n",
            "Requirement already satisfied: dopamine-rl in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (1.0.5)\n",
            "Requirement already satisfied: tf-slim in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (1.1.0)\n",
            "Requirement already satisfied: gunicorn in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (20.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (3.1.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (1.12.8)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (4.1.3)\n",
            "Requirement already satisfied: kfac in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (0.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (1.4.1)\n",
            "Requirement already satisfied: tensorflow-gan in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (2.1.0)\n",
            "Requirement already satisfied: pypng in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (0.0.21)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (7.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (0.12.0)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (0.98)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (1.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (1.19.5)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (0.5.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (4.1.2.30)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (0.17.3)\n",
            "Requirement already satisfied: tensorflow-probability==0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (0.7.0)\n",
            "Requirement already satisfied: cloudpickle>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor) (4.4.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->tensor2tensor) (1.5.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->tensor2tensor) (2.0.1)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.7/dist-packages (from gevent->tensor2tensor) (4.5.0)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.7/dist-packages (from gevent->tensor2tensor) (5.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent->tensor2tensor) (57.4.0)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent->tensor2tensor) (1.1.2)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor) (1.26.3)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor) (1.35.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor) (0.17.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor) (3.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor) (1.53.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor) (3.17.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor) (21.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client->tensor2tensor) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client->tensor2tensor) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client->tensor2tensor) (0.2.8)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client->tensor2tensor) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor) (3.0.4)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->tensor2tensor) (1.5.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->tensor2tensor) (1.2.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tensor2tensor) (2.7.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (0.1.6)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (1.4.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (2.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (21.2.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (5.4.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (0.3.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow-datasets->tensor2tensor) (3.6.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gan->tensor2tensor) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc0kAZgQrG6E"
      },
      "source": [
        "# U2GNN Tensorflow (unfinished)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0guCn8_1naw"
      },
      "source": [
        "def universal_transformer_encoder(encoder_input, encoder_self_attention_bias, hparams, name=\"encoder\", nonpadding=None, save_weights_to=None, make_image_summary=True):\n",
        "  x = encoder_input\n",
        "  attention_dropout_broadcast_dims = (\n",
        "      common_layers.comma_separated_string_to_integer_list(\n",
        "          getattr(hparams, \"attention_dropout_broadcast_dims\", \"\")))\n",
        "  with tf.variable_scope(name):\n",
        "    if nonpadding is not None:\n",
        "      padding = 1.0 - nonpadding\n",
        "    else:\n",
        "      padding = common_attention.attention_bias_to_padding(\n",
        "          encoder_self_attention_bias)\n",
        "      nonpadding = 1.0 - padding\n",
        "    pad_remover = None\n",
        "    if hparams.use_pad_remover and not common_layers.is_xla_compiled():\n",
        "      pad_remover = expert_utils.PadRemover(padding)\n",
        "\n",
        "    ffn_unit = functools.partial(\n",
        "        transformer_encoder_ffn_unit,\n",
        "        hparams=hparams,\n",
        "        nonpadding_mask=nonpadding,\n",
        "        pad_remover=pad_remover)\n",
        "\n",
        "    attention_unit = functools.partial(\n",
        "        transformer_encoder_attention_unit,\n",
        "        hparams=hparams,\n",
        "        encoder_self_attention_bias=encoder_self_attention_bias,\n",
        "        attention_dropout_broadcast_dims=attention_dropout_broadcast_dims,\n",
        "        save_weights_to=save_weights_to,\n",
        "        make_image_summary=make_image_summary)\n",
        "\n",
        "    x, extra_output = universal_transformer_layer(\n",
        "        x, hparams, ffn_unit, attention_unit, pad_remover=pad_remover)\n",
        "\n",
        "    if hparams.get(\"use_memory_as_last_state\", False):\n",
        "      x = extra_output  # which is memory\n",
        "    return common_layers.layer_preprocess(x, hparams), extra_output\n",
        "\n",
        "\n",
        "def universal_transformer_decoder(decoder_input, encoder_output, decoder_self_attention_bias, encoder_decoder_attention_bias, hparams, name=\"decoder\", nonpadding=None, save_weights_to=None, make_image_summary=True):\n",
        "  x = decoder_input\n",
        "  attention_dropout_broadcast_dims = (\n",
        "      common_layers.comma_separated_string_to_integer_list(\n",
        "          getattr(hparams, \"attention_dropout_broadcast_dims\", \"\")))\n",
        "  with tf.variable_scope(name):\n",
        "    ffn_unit = functools.partial(\n",
        "        transformer_decoder_ffn_unit,\n",
        "        hparams=hparams,\n",
        "        nonpadding_mask=nonpadding)\n",
        "\n",
        "    attention_unit = functools.partial(\n",
        "        transformer_decoder_attention_unit,\n",
        "        hparams=hparams,\n",
        "        encoder_output=encoder_output,\n",
        "        decoder_self_attention_bias=decoder_self_attention_bias,\n",
        "        encoder_decoder_attention_bias=encoder_decoder_attention_bias,\n",
        "        attention_dropout_broadcast_dims=attention_dropout_broadcast_dims,\n",
        "        save_weights_to=save_weights_to,\n",
        "        make_image_summary=make_image_summary)\n",
        "\n",
        "    x, extra_output = universal_transformer_layer(\n",
        "        x, hparams, ffn_unit, attention_unit)\n",
        "\n",
        "    return common_layers.layer_preprocess(x, hparams), extra_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I13pjPayxCFr"
      },
      "source": [
        "def universal_transformer_encoder(encoder_input, encoder_self_attention_bias, hparams, name=\"encoder\", nonpadding=None, save_weights_to=None, make_image_summary=True):\n",
        "  x = encoder_input\n",
        "  attention_dropout_broadcast_dims = (common_layers.comma_separated_string_to_integer_list(getattr(hparams, \"attention_dropout_broadcast_dims\", \"\")))\n",
        "  with tf.variable_scope(name):\n",
        "    if nonpadding is not None:\n",
        "      padding = 1.0 - nonpadding\n",
        "    else:\n",
        "      padding = common_attention.attention_bias_to_padding(encoder_self_attention_bias)\n",
        "      nonpadding = 1.0 - padding\n",
        "    pad_remover = None\n",
        "    if hparams.use_pad_remover and not common_layers.is_xla_compiled():\n",
        "      pad_remover = expert_utils.PadRemover(padding)\n",
        "    ffn_unit = functools.partial(transformer_encoder_ffn_unit,hparams=hparams,nonpadding_mask=nonpadding,pad_remover=pad_remover)\n",
        "    attention_unit = functools.partial(\n",
        "        transformer_encoder_attention_unit,\n",
        "        hparams=hparams,\n",
        "        encoder_self_attention_bias=encoder_self_attention_bias,\n",
        "        attention_dropout_broadcast_dims=attention_dropout_broadcast_dims,\n",
        "        save_weights_to=save_weights_to,\n",
        "        make_image_summary=make_image_summary)\n",
        "    x, extra_output = universal_transformer_layer(x, hparams, ffn_unit, attention_unit, pad_remover=pad_remover)\n",
        "    if hparams.get(\"use_memory_as_last_state\", False):\n",
        "      x = extra_output  # which is memory\n",
        "    return common_layers.layer_preprocess(x, hparams), extra_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "7bSBkc97rXQq",
        "outputId": "81f1a25f-dfea-426c-db8a-506fd5dda559"
      },
      "source": [
        "class UniversalTransformer1(transformer.Transformer):\n",
        "\n",
        "  def encode(self, inputs, target_space, hparams, features=None, losses=None):\n",
        "    del losses\n",
        "    inputs = common_layers.flatten4d3d(inputs)\n",
        "    encoder_input, self_attention_bias, encoder_decoder_attention_bias = (transformer.transformer_prepare_encoder(inputs, target_space, hparams, features=features))\n",
        "    encoder_input = tf.nn.dropout(encoder_input, 1.0 - hparams.layer_prepostprocess_dropout)\n",
        "    (encoder_output, encoder_extra_output) = (universal_transformer_encoder(encoder_input,self_attention_bias, hparams, nonpadding=transformer.features_to_nonpadding(features, \"inputs\"), save_weights_to=self.attention_weights))\n",
        "    return encoder_output, encoder_decoder_attention_bias, encoder_extra_output\n",
        "\n",
        "  def decode(self, decoder_input, encoder_output, encoder_decoder_attention_bias, decoder_self_attention_bias, hparams, cache=None, decode_loop_step=None, nonpadding=None, losses=None):\n",
        "    del decode_loop_step\n",
        "    del losses\n",
        "    del cache\n",
        "    decoder_input = tf.nn.dropout(decoder_input, 1.0 - hparams.layer_prepostprocess_dropout)\n",
        "    (decoder_output, dec_extra_output) = (universal_transformer_decoder(decoder_input, encoder_output, decoder_self_attention_bias, encoder_decoder_attention_bias, hparams, nonpadding=nonpadding, save_weights_to=self.attention_weights))\n",
        "    return tf.expand_dims(decoder_output, axis=2), dec_extra_output\n",
        "\n",
        "  def body(self, features):\n",
        "    hparams = self._hparams\n",
        "    if hparams.add_position_timing_signal:\n",
        "      hparams.pos = None\n",
        "    if self.has_input:\n",
        "      inputs = features[\"inputs\"]\n",
        "      target_space = features[\"target_space_id\"]\n",
        "      (encoder_output, encoder_decoder_attention_bias, enc_extra_output) = self.encode(inputs, target_space, hparams, features=features)\n",
        "    else:\n",
        "      (encoder_output, encoder_decoder_attention_bias, enc_extra_output) = (None, None, (None, None))\n",
        "    targets = features[\"targets\"]\n",
        "    targets = common_layers.flatten4d3d(targets)\n",
        "    (decoder_input, decoder_self_attention_bias) = transformer.transformer_prepare_decoder(targets, hparams, features=features)\n",
        "    decoder_output, dec_extra_output = self.decode(decoder_input, encoder_output, encoder_decoder_attention_bias, decoder_self_attention_bias, hparams, nonpadding=transformer.features_to_nonpadding(features, \"targets\"))\n",
        "    expected_attentions = features.get(\"expected_attentions\")\n",
        "    if expected_attentions is not None:\n",
        "      attention_loss = common_attention.encoder_decoder_attention_loss(expected_attentions, self.attention_weights, hparams.expected_attention_loss_type, hparams.expected_attention_loss_multiplier)\n",
        "      return decoder_output, {\"attention_loss\": attention_loss}\n",
        "    if hparams.recurrence_type == \"act\" and hparams.act_loss_weight != 0:\n",
        "      if self.has_input:\n",
        "        enc_ponder_times, enc_remainders = enc_extra_output\n",
        "        enc_act_loss = (hparams.act_loss_weight * tf.reduce_mean(enc_ponder_times + enc_remainders))\n",
        "      else:\n",
        "        enc_act_loss = 0.0\n",
        "      (dec_ponder_times, dec_remainders) = dec_extra_output\n",
        "      dec_act_loss = (hparams.act_loss_weight * tf.reduce_mean(dec_ponder_times + dec_remainders))\n",
        "      act_loss = enc_act_loss + dec_act_loss\n",
        "      tf.contrib.summary.scalar(\"act_loss\", act_loss)\n",
        "      return decoder_output, {\"act_loss\": act_loss}\n",
        "    return decoder_output\n",
        "\n",
        "  def _greedy_infer(self, features, decode_length, use_tpu=False):\n",
        "    return (self._slow_greedy_infer_tpu(features, decode_length) if use_tpu else self._slow_greedy_infer(features, decode_length))\n",
        "\n",
        "  def _beam_decode(self, features, decode_length, beam_size, top_beams, alpha):\n",
        "    return self._beam_decode_slow(features, decode_length, beam_size, top_beams, alpha)\n",
        "    \n",
        "@registry.register_model\n",
        "class UniversalTransformerEncoder1(transformer.Transformer):\n",
        "  def encode(self, inputs, target_space, hparams, features=None, losses=None):\n",
        "    del losses\n",
        "    inputs = common_layers.flatten4d3d(inputs)\n",
        "    (encoder_input, self_attention_bias, _) = (transformer.transformer_prepare_encoder(inputs, target_space, hparams))\n",
        "    encoder_input = tf.nn.dropout(encoder_input, 1.0 - hparams.layer_prepostprocess_dropout)\n",
        "    (encoder_output, encoder_extra_output) = (universal_transformer_encoder(encoder_input,self_attention_bias, hparams, nonpadding=transformer.features_to_nonpadding(features, \"inputs\"), save_weights_to=self.attention_weights))\n",
        "    return encoder_output, encoder_extra_output\n",
        "\n",
        "  def body(self, features):\n",
        "    hparams = self._hparams\n",
        "    inputs = features[\"inputs\"]\n",
        "    target_space = features[\"target_space_id\"]\n",
        "    encoder_output, enc_extra_output = self.encode(inputs, target_space, hparams, features=features)\n",
        "    encoder_output = tf.expand_dims(encoder_output, 2)\n",
        "    if hparams.recurrence_type == \"act\" and hparams.act_loss_weight != 0:\n",
        "      ponder_times, remainders = enc_extra_output\n",
        "      act_loss = hparams.act_loss_weight * tf.reduce_mean(ponder_times + remainders)\n",
        "      tf.contrib.summary.scalar(\"act_loss\", act_loss)\n",
        "      return encoder_output, {\"act_loss\": act_loss}\n",
        "    return encoder_output\n",
        "\n",
        "def update_hparams_for_universal_transformer(hparams):\n",
        "  hparams.daisy_chain_variables = False \n",
        "  hparams.add_hparam(\"mix_with_transformer\", None)\n",
        "  hparams.add_hparam(\"num_mixedin_layers\", 2)\n",
        "  hparams.add_hparam(\"recurrence_type\", \"basic\")\n",
        "  hparams.add_hparam(\"num_rec_steps\", hparams.num_hidden_layers)\n",
        "  hparams.add_hparam(\"add_position_timing_signal\", True)\n",
        "  if hparams.add_position_timing_signal:\n",
        "    hparams.pos = None\n",
        "  hparams.add_hparam(\"position_start_index\", None)\n",
        "  hparams.add_hparam(\"add_step_timing_signal\", True)\n",
        "  hparams.add_hparam(\"step_timing_signal_type\", \"learned\")\n",
        "  hparams.add_hparam(\"add_or_concat_timing_signal\", \"add\")\n",
        "  hparams.add_hparam(\"add_sru\", False)\n",
        "  hparams.add_hparam(\"transformer_ffn_type\", \"fc\")\n",
        "  hparams.add_hparam(\"transform_bias_init\", -1.0)\n",
        "  hparams.add_hparam(\"couple_carry_transform_gates\", True)\n",
        "  hparams.add_hparam(\"depth_embedding\", True)\n",
        "  hparams.add_hparam(\"dwa_elements\", True)\n",
        "  hparams.add_hparam(\"gate_ffn_layer\", \"dense\")\n",
        "  hparams.add_hparam(\"lstm_forget_bias\", 1.0)\n",
        "  hparams.add_hparam(\"use_memory_as_final_state\", True)\n",
        "  hparams.add_hparam(\"add_ffn_unit_to_the_transition_function\", False)\n",
        "  hparams.add_hparam(\"act_type\", \"basic\")\n",
        "  hparams.add_hparam(\"act_max_steps\", 2 * hparams.num_hidden_layers)\n",
        "  hparams.add_hparam(\"act_halting_bias_init\", 1.0)\n",
        "  hparams.add_hparam(\"act_epsilon\", 0.01)\n",
        "  hparams.add_hparam(\"act_loss_weight\", 0.01)\n",
        "  return hparams\n",
        "\n",
        "@registry.register_hparams\n",
        "def universal_transformer_small1():\n",
        "  hparams = transformer.transformer_base()\n",
        "  hparams = update_hparams_for_universal_transformer(hparams)\n",
        "  return hparams"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-522392e516d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_beam_decode_slow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_beams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mUniversalTransformerEncoder1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'registry' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12pJya6JrN7Q"
      },
      "source": [
        "class UGformerV1(object):\n",
        "    def __init__(self, feature_dim_size, hparams_batch_size, ff_hidden_size, seq_length, num_classes, num_self_att_layers, num_GNN_layers=1):\n",
        "        self.input_x = tf.compat.v1.placeholder(tf.int32, [None, seq_length], name=\"input_x\")\n",
        "        self.graph_pool = tf.compat.v1.sparse_placeholder(tf.float32, [None, None], name=\"graph_pool\")\n",
        "        self.X_concat = tf.compat.v1.placeholder(tf.float32, [None, feature_dim_size], name=\"X_concat\")\n",
        "        self.one_hot_labels = tf.compat.v1.placeholder(tf.float32, [None, num_classes], name=\"one_hot_labels\")\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
        "\n",
        "        #Inputs for Universal Transformer\n",
        "        self.input_UT = tf.nn.embedding_lookup(self.X_concat, self.input_x)\n",
        "        self.input_UT = tf.reshape(self.input_UT, [-1, seq_length, 1, feature_dim_size])\n",
        "\n",
        "        #Matrix weights in Universal Transformer are shared across each attention layer (timestep), while they are not in Transformer.\n",
        "        #It's optional to use Transformer Encoder.\n",
        "        self.hparams = universal_transformer_small1()\n",
        "        self.hparams.hidden_size = feature_dim_size\n",
        "        self.hparams.batch_size = hparams_batch_size * seq_length\n",
        "        self.hparams.max_length = seq_length\n",
        "        self.hparams.num_hidden_layers = num_self_att_layers  # Number of attention layers: the number T of timesteps in Universal Transformer\n",
        "        self.hparams.num_heads = 1  # due to the fact that the feature embedding sizes are various\n",
        "        self.hparams.filter_size = ff_hidden_size\n",
        "        self.hparams.use_target_space_embedding = False\n",
        "        self.hparams.pos = None\n",
        "        self.hparams.add_position_timing_signal = False\n",
        "        self.hparams.add_step_timing_signal = False\n",
        "        self.hparams.add_sru = False\n",
        "        self.hparams.add_or_concat_timing_signal = None\n",
        "\n",
        "        # Construct k GNN layers\n",
        "        self.scores = 0\n",
        "        for layer in range(num_GNN_layers):  # the number k of multiple stacked layers, each stacked layer includes a number of self-attention layers\n",
        "            # Universal Transformer Encoder\n",
        "            self.ute = universal_transformer_modified.UniversalTransformerEncoder1(self.hparams, mode=tf.estimator.ModeKeys.TRAIN)\n",
        "            self.output_UT = self.ute({\"inputs\": self.input_UT, \"targets\": 0, \"target_space_id\": 0})[0]\n",
        "            self.output_UT = tf.squeeze(self.output_UT, axis=2)\n",
        "            #\n",
        "            self.output_target_node = tf.split(self.output_UT, num_or_size_splits=seq_length, axis=1)[0]\n",
        "            self.output_target_node = tf.squeeze(self.output_target_node, axis=1)\n",
        "            #input for next GNN hidden layer\n",
        "            self.input_UT = tf.nn.embedding_lookup(self.output_target_node, self.input_x)\n",
        "            self.input_UT = tf.reshape(self.input_UT, [-1, seq_length, 1, feature_dim_size])\n",
        "            # graph pooling\n",
        "            self.graph_embeddings = tf.compat.v1.sparse_tensor_dense_matmul(self.graph_pool, self.output_target_node)\n",
        "            self.graph_embeddings = tf.nn.dropout(self.graph_embeddings, keep_prob=self.dropout_keep_prob)\n",
        "\n",
        "            # Concatenate graph representations from all GNN layers\n",
        "            with tf.variable_scope(\"layer_%d\" % layer):\n",
        "                W = tf.compat.v1.get_variable(shape=[feature_dim_size, num_classes],\n",
        "                                              initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                                              name=\"W_layer_%d\" % layer)\n",
        "                b = tf.Variable(tf.zeros([num_classes]))\n",
        "                self.scores += tf.compat.v1.nn.xw_plus_b(self.graph_embeddings, W, b)\n",
        "\n",
        "        # Final predictions\n",
        "        self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "        # Calculate mean cross-entropy loss\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            losses = tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(logits=self.scores, labels=label_smoothing(self.one_hot_labels))\n",
        "            self.total_loss = tf.reduce_mean(losses)\n",
        "\n",
        "        # Accuracy\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.one_hot_labels, 1))\n",
        "            self.accuracy = tf.reduce_sum(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
        "\n",
        "        self.saver = tf.compat.v1.train.Saver(tf.global_variables(), max_to_keep=500)\n",
        "        tf.logging.info('Seting up the main structure')\n",
        "\n",
        "def label_smoothing(inputs, epsilon=0.1):\n",
        "    V = inputs.get_shape().as_list()[-1]  # number of channels\n",
        "    return ((1 - epsilon) * inputs) + (epsilon / V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWvlKPVLmVwm"
      },
      "source": [
        "# U2GNN Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnsKjcDNmYnW"
      },
      "source": [
        "class UGformerV1(nn.Module):\n",
        "  def __init__(self, feature_dim_size, ff_hidden_size, num_classes, num_self_att_layers, dropout, num_GNN_layers):\n",
        "    super(UGformerV1, self).__init__()\n",
        "    self.feature_dim_size = feature_dim_size\n",
        "    self.ff_hidden_size = ff_hidden_size\n",
        "    self.num_classes = num_classes\n",
        "    self.num_self_att_layers = num_self_att_layers\n",
        "    self.num_GNN_layers = num_GNN_layers\n",
        "    self.ugformer_layers = torch.nn.ModuleList()\n",
        "    for _ in range(self.num_GNN_layers):\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=self.feature_dim_size, nhead=1, dim_feedforward=self.ff_hidden_size, dropout=0.5)\n",
        "        self.ugformer_layers.append(TransformerEncoder(encoder_layers, self.num_self_att_layers))\n",
        "    self.predictions = torch.nn.ModuleList()\n",
        "    self.dropouts = torch.nn.ModuleList()\n",
        "    for _ in range(self.num_GNN_layers):\n",
        "        self.predictions.append(nn.Linear(self.feature_dim_size, self.num_classes))\n",
        "        self.dropouts.append(nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, input_x, graph_pool, X_concat):\n",
        "    prediction_scores = 0\n",
        "    input_Tr = F.embedding(input_x, X_concat)\n",
        "    for layer_idx in range(self.num_GNN_layers):\n",
        "        output_Tr = self.ugformer_layers[layer_idx](input_Tr)[0]\n",
        "        input_Tr = F.embedding(input_x, output_Tr)\n",
        "        graph_embeddings = torch.spmm(graph_pool, output_Tr)\n",
        "        graph_embeddings = self.dropouts[layer_idx](graph_embeddings)\n",
        "        prediction_scores += self.predictions[layer_idx](graph_embeddings)\n",
        "        return prediction_scores\n",
        "\n",
        "def label_smoothing(true_labels: torch.Tensor, classes: int, smoothing=0.1):\n",
        "    assert 0 <= smoothing < 1\n",
        "    confidence = 1.0 - smoothing\n",
        "    label_shape = torch.Size((true_labels.size(0), classes))\n",
        "    with torch.no_grad():\n",
        "        true_dist = torch.empty(size=label_shape, device=true_labels.device)\n",
        "        true_dist.fill_(smoothing / (classes - 1))\n",
        "        true_dist.scatter_(1, true_labels.data.unsqueeze(1), confidence)\n",
        "    return true_dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZADJTMIwOpzb"
      },
      "source": [
        "# Email preprocessing\n",
        "\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GulpviYhPDCI"
      },
      "source": [
        "NODE_EMBEDDING_LENGTH = 1280\n",
        "text_encoder = SentenceTransformer('distiluse-base-multilingual-cased', device='cuda')\n",
        "attachment_encoder = tf.keras.applications.MobileNetV3Small(weights='imagenet', input_shape=(224, 224, 3), include_top = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0pPmulaPw1B"
      },
      "source": [
        "class S2VGraph(object):\n",
        "    def __init__(self):\n",
        "        self.label = 0\n",
        "        self.g = 0\n",
        "        self.neighbors = []\n",
        "        self.node_features = 0\n",
        "        self.edge_mat = 0\n",
        "        self.max_neighbour = 0\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lxaWlLEOuCh"
      },
      "source": [
        "from email.iterators import _structure\n",
        "from contextlib import redirect_stdout\n",
        "import io\n",
        "\n",
        "def tag_visible(element):\n",
        "  if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]'] or isinstance(element, Comment):\n",
        "      return False\n",
        "  else:\n",
        "      return True\n",
        "\n",
        "def process_html(txt):\n",
        "  soup = BeautifulSoup(txt, 'html.parser')\n",
        "  texts = soup.findAll(text=True)\n",
        "  visible_texts = filter(tag_visible, texts)\n",
        "  return ' '.join(visible_texts) \n",
        "\n",
        "def get_structure(msg):\n",
        "  with io.StringIO() as buf, redirect_stdout(buf):\n",
        "    _structure(msg)\n",
        "    ans = buf.getvalue()\n",
        "  return ans.replace(\"    \", '\\t').split('\\n')\n",
        "\n",
        "def encode_text(part):\n",
        "  text = part.get_payload(decode=True).decode('latin-1')\n",
        "  if bool(BeautifulSoup(text, \"html.parser\").find()):\n",
        "    text = process_html(text)\n",
        "  return text_encoder.encode(text)\n",
        "\n",
        "def encode_image(part):\n",
        "  if part.get_content_subtype() != 'gif':\n",
        "    with open('/content/attachment.' + part.get_content_subtype(), \"wb+\") as f:\n",
        "      f.write(part.get_payload(decode=True))\n",
        "    try:\n",
        "      image = cv2.imread('/content/attachment.' + art.get_content_subtype())\n",
        "      image = cv2.resize(image, (224, 224))\n",
        "      image = np.reshape(image, (1, 224, 224, 3))\n",
        "    except Exception:\n",
        "      return np.zeros(NODE_EMBEDDING_LENGTH)\n",
        "    return attachment_encoder.predict(image)\n",
        "  return np.zeros(NODE_EMBEDDING_LENGTH)\n",
        "\n",
        "def get_node_features(part):\n",
        "    if part.get_content_maintype() == 'text' or part.get_content_type() == 'plain':\n",
        "      features = encode_text(part)\n",
        "    elif part.get_content_maintype() == 'image':\n",
        "      features = encode_image(part)\n",
        "    else:\n",
        "      features = np.zeros(NODE_EMBEDDING_LENGTH)\n",
        "    return np.pad(features, NODE_EMBEDDING_LENGTH - features.shape[0])\n",
        "      \n",
        "\n",
        "def process_email(msg, label):\n",
        "  struct = get_structure(msg)\n",
        "  g = nx.Graph()\n",
        "  cur_graph = S2VGraph()\n",
        "  node_idx = 0\n",
        "  stack = []\n",
        "  node_features = np.zeros((len(struct), NODE_EMBEDDING_LENGTH))\n",
        "  for part in msg.walk():\n",
        "    g.add_node(node_idx, payload=part.get_content_type()) \n",
        "    cur_depth = struct[node_idx].count('\\t')\n",
        "    while len(stack) > cur_depth:\n",
        "      stack.pop()\n",
        "    if len(stack) != 0:\n",
        "      parent_idx = stack.pop()\n",
        "      g.add_edge(parent_idx, node_idx)\n",
        "      stack.append(parent_idx)\n",
        "    stack.append(node_idx)\n",
        "    node_idx += 1\n",
        "    node_features[node_idx, :] = get_node_features(part)\n",
        "  cur_graph.g = g\n",
        "  cur_graph.label = label\n",
        "  cur_graph.node_features = node_features\n",
        "  cur_graph.neighbours = [[] for i in range(len(cur_graph.g))]\n",
        "  for i, j in cur_graph.g.edges():\n",
        "    cur_graph.neighbours[i].append(j)\n",
        "    cur_graph.neighbours[j].append(i)\n",
        "  degree_list = []\n",
        "  for i in range(len(cur_graph.g)):\n",
        "    degree_list.append(len(cur_graph.neighbours[i]))\n",
        "  cur_graph.max_neighbour = max(degree_list)\n",
        "  edges = [list(pair) for pair in cur_graph.g.edges()]\n",
        "  edges.extend([[i, j] for j, i in edges])\n",
        "  deg_list = list(dict(cur_graph.g.degree(range(len(cur_graph.g)))).values())\n",
        "  cur_graph.edge_mat = np.transpose(np.array(edges, dtype=np.int32), (1,0))\n",
        "  return cur_graph\n",
        "\n",
        "\n",
        "def get_processed_arr(raw_emails, classes):\n",
        "  input_graphs = []\n",
        "  for i in range(len(raw_emails)):\n",
        "    input_graphs.append(process_email(email.message_from_string(raw_emails[i])), classes[i])\n",
        "  return input_graphs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZxRw8VdObno"
      },
      "source": [
        "# SpamAssassin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lk7yXuJOaRc"
      },
      "source": [
        "spamassassin = zipfile.ZipFile('/content/drive/MyDrive/Colab Notebooks/Coursework/datasets/spamassassin.zip', 'r')\n",
        "spamassassin.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxI85mOWOiSx"
      },
      "source": [
        "def extract_emails_spamassassin():\n",
        "  spamdir = '/content/spam_2/spam_2'\n",
        "  ehamdir = '/content/easy_ham/easy_ham'\n",
        "  hhamdir = '/content/hard_ham/hard_ham'\n",
        "  nspam = len(os.listdir(spamdir))\n",
        "  neham = len(os.listdir(ehamdir))\n",
        "  nhham = len(os.listdir(hhamdir))\n",
        "  sa_classes = np.concatenate((np.ones(nspam), np.zeros(neham + nhham)))\n",
        "  sa_texts = np.empty(nspam + nhham + neham, dtype=object)\n",
        "  for i, filename in enumerate(os.listdir(spamdir)):\n",
        "    curr_email = open(os.path.join(spamdir, filename), encoding='latin-1')\n",
        "    sa_texts[i] = curr_email.read()\n",
        "  for i, filename in enumerate(os.listdir(ehamdir)):\n",
        "    curr_email = open(os.path.join(ehamdir, filename), encoding='latin-1')\n",
        "    sa_texts[i + nspam] = curr_email.read()\n",
        "  for i, filename in enumerate(os.listdir(hhamdir)):\n",
        "    curr_email = open(os.path.join(hhamdir, filename), encoding='latin-1')\n",
        "    sa_texts[i + nspam + neham] = curr_email.read()\n",
        "  return sa_texts, sa_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0MAiSl2OjOJ"
      },
      "source": [
        "sa_raw_texts, sa_classes = extract_emails_spamassassin()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "PngAvLRri6TO",
        "outputId": "b46c3854-ba52-494d-dad8-d620ee328cd5"
      },
      "source": [
        "graph = process_email(email.message_from_string(sa_raw_texts[3]), sa_classes[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-418cf7b3f878>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_email\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa_raw_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-c05a66a0067b>\u001b[0m in \u001b[0;36mprocess_email\u001b[0;34m(msg, label)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mnode_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mnode_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_node_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m   \u001b[0mcur_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m   \u001b[0mcur_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-c05a66a0067b>\u001b[0m in \u001b[0;36mget_node_features\u001b[0;34m(part)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_node_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_content_maintype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'text'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_content_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plain'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m       \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_content_maintype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-c05a66a0067b>\u001b[0m in \u001b[0;36mencode_text\u001b[0;34m(part)\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtext_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mall_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "QQFUPxkyrLPx",
        "outputId": "255caff8-946c-4e31-d41c-ef0039a41660"
      },
      "source": [
        "nx.draw(graph.g, labels=nx.get_node_attributes(graph.g, \"payload\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AMTf5YQrZEH",
        "outputId": "ea01a869-e8bd-4265-9997-d906279f7272"
      },
      "source": [
        "_structure(email.message_from_string(sa_raw_texts[3]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multipart/alternative\n",
            "    text/plain\n",
            "    text/html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-SK6HJPiwUH",
        "outputId": "444a99f8-82e0-4bee-e9d4-52c0b8f8131c"
      },
      "source": [
        "print(tmp.strip().split('\\n'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['multipart/alternative', '\\ttext/plain', '\\ttext/html']\n"
          ]
        }
      ]
    }
  ]
}